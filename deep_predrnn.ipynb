{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY_ILf4kXbu1",
        "outputId": "13f869f0-0bfc-4bb1-9539-d2b2512a8d32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d matthewjansen/ucf101-action-recognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J5TS2DuXkjG",
        "outputId": "dc7fad11-52fe-4a94-8606-73fc2407444e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/matthewjansen/ucf101-action-recognition\n",
            "License(s): CC0-1.0\n",
            "Downloading ucf101-action-recognition.zip to /content\n",
            "100% 6.52G/6.53G [04:56<00:00, 22.4MB/s]\n",
            "100% 6.53G/6.53G [04:56<00:00, 23.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "# /content/ucf101-action-recognition.zip\n",
        "# Path to the zip file\n",
        "zip_file_path = '/content/ucf101-action-recognition.zip'\n",
        "extracted_path = '/content/ucf101-action-recognition'\n",
        "\n",
        "# Unzipping the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_path)\n",
        "\n",
        "# Check if extraction was successful\n",
        "extracted_files = os.listdir(extracted_path)\n",
        "print(\"Files and directories after extraction:\")\n",
        "for file in extracted_files:\n",
        "    print(file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_GohP5wXpMj",
        "outputId": "cc589da4-3090-4994-9025-ec3c6ea47ead"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files and directories after extraction:\n",
            "val\n",
            "val.csv\n",
            "train\n",
            "test\n",
            "train.csv\n",
            "test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "extracted_path = './ucf101-action-recognition'\n",
        "\n",
        "# Path to the CSV files\n",
        "train_csv_path = os.path.join(extracted_path, 'train.csv')\n",
        "val_csv_path = os.path.join(extracted_path, 'val.csv')\n",
        "\n",
        "# Load the CSV files\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "val_df = pd.read_csv(val_csv_path)\n",
        "\n",
        "# Display the first few rows of train.csv to understand its structure\n",
        "print(\"Training Data Sample:\")\n",
        "print(train_df.head())\n",
        "\n",
        "# Filter for the selected classes\n",
        "selected_classes = ['Biking', 'SoccerPenalty', 'JumpingJack', 'BasketballDunk', 'VolleyballSpiking']\n",
        "\n",
        "train_selected = train_df[train_df['label'].isin(selected_classes)]\n",
        "val_selected = val_df[val_df['label'].isin(selected_classes)]\n",
        "\n",
        "# Display filtered train and val datasets\n",
        "print(\"\\nFiltered Training Data:\")\n",
        "print(train_selected.head())\n",
        "\n",
        "print(\"\\nFiltered Validation Data:\")\n",
        "print(val_selected.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6B3eN9wZToT",
        "outputId": "797f5987-75ef-4e45-ceb2-856c4c82fdf5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Sample:\n",
            "         clip_name                         clip_path  label\n",
            "0  v_Swing_g05_c02  /train/Swing/v_Swing_g05_c02.avi  Swing\n",
            "1  v_Swing_g21_c03  /train/Swing/v_Swing_g21_c03.avi  Swing\n",
            "2  v_Swing_g07_c01  /train/Swing/v_Swing_g07_c01.avi  Swing\n",
            "3  v_Swing_g24_c04  /train/Swing/v_Swing_g24_c04.avi  Swing\n",
            "4  v_Swing_g20_c03  /train/Swing/v_Swing_g20_c03.avi  Swing\n",
            "\n",
            "Filtered Training Data:\n",
            "                  clip_name                                     clip_path  \\\n",
            "1282  v_JumpingJack_g13_c03  /train/JumpingJack/v_JumpingJack_g13_c03.avi   \n",
            "1283  v_JumpingJack_g14_c03  /train/JumpingJack/v_JumpingJack_g14_c03.avi   \n",
            "1284  v_JumpingJack_g25_c02  /train/JumpingJack/v_JumpingJack_g25_c02.avi   \n",
            "1285  v_JumpingJack_g01_c04  /train/JumpingJack/v_JumpingJack_g01_c04.avi   \n",
            "1286  v_JumpingJack_g06_c06  /train/JumpingJack/v_JumpingJack_g06_c06.avi   \n",
            "\n",
            "            label  \n",
            "1282  JumpingJack  \n",
            "1283  JumpingJack  \n",
            "1284  JumpingJack  \n",
            "1285  JumpingJack  \n",
            "1286  JumpingJack  \n",
            "\n",
            "Filtered Validation Data:\n",
            "                 clip_name                                   clip_path  \\\n",
            "215  v_JumpingJack_g14_c01  /val/JumpingJack/v_JumpingJack_g14_c01.avi   \n",
            "216  v_JumpingJack_g21_c03  /val/JumpingJack/v_JumpingJack_g21_c03.avi   \n",
            "217  v_JumpingJack_g05_c02  /val/JumpingJack/v_JumpingJack_g05_c02.avi   \n",
            "218  v_JumpingJack_g05_c03  /val/JumpingJack/v_JumpingJack_g05_c03.avi   \n",
            "219  v_JumpingJack_g11_c01  /val/JumpingJack/v_JumpingJack_g11_c01.avi   \n",
            "\n",
            "           label  \n",
            "215  JumpingJack  \n",
            "216  JumpingJack  \n",
            "217  JumpingJack  \n",
            "218  JumpingJack  \n",
            "219  JumpingJack  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Path for saving frames\n",
        "frame_save_path = '/content/frames_raw/'\n",
        "\n",
        "# Ensure the directory for saving frames exists\n",
        "os.makedirs(frame_save_path, exist_ok=True)\n",
        "\n",
        "# Function to extract frames from a video\n",
        "def extract_frames(video_path, class_name, clip_name, save_path, frame_size=(64, 64)):\n",
        "    # Create a directory for the class if it doesn't exist\n",
        "    class_save_path = os.path.join(save_path, class_name)\n",
        "    os.makedirs(class_save_path, exist_ok=True)\n",
        "\n",
        "    # Create a directory for the specific video clip\n",
        "    clip_save_path = os.path.join(class_save_path, clip_name)\n",
        "    os.makedirs(clip_save_path, exist_ok=True)\n",
        "\n",
        "    # Open video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Read frames and save them\n",
        "    frame_num = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Resize the frame\n",
        "        frame_resized = cv2.resize(frame, frame_size)\n",
        "\n",
        "        # Convert to grayscale (optional)\n",
        "        frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Save frame as an image\n",
        "        frame_filename = f\"{clip_name}_frame_{frame_num:03d}.jpg\"\n",
        "        cv2.imwrite(os.path.join(clip_save_path, frame_filename), frame_gray)\n",
        "\n",
        "        frame_num += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "# Example: Extract frames from videos for the 'JumpingJack' class\n",
        "# for class_name in ['Biking', 'SoccerPenalty', 'JumpingJack', 'BasketballDunk', 'VolleyballSpiking']:\n",
        "#     for video in train_df[train_df['label'] == class_name]['clip_path']:\n",
        "#         video_filename = video#.split('/')[-1]#.replace('.avi', '')\n",
        "#         print(f\"Extracting frames for {class_name} - {video_filename}\")\n",
        "#         extract_frames('./ucf101-action-recognition'+video, class_name, video_filename, frame_save_path)\n",
        "for class_name in ['Biking', 'SoccerPenalty', 'JumpingJack', 'BasketballDunk', 'VolleyballSpiking']:\n",
        "    for video in train_df[train_df['label'] == class_name]['clip_path']:\n",
        "        video_filename = video.split('/')[-1].replace('.avi', '')\n",
        "        # print(f\"Extracting and augmenting frames for {class_name} - {video_filename}\")\n",
        "        extract_frames('/content/ucf101-action-recognition' + video, class_name, video_filename, frame_save_path)"
      ],
      "metadata": {
        "id": "skM7l0vaX26V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame_save_path = '/content/frames_val/'\n",
        "\n",
        "for class_name in ['Biking', 'SoccerPenalty', 'JumpingJack', 'BasketballDunk', 'VolleyballSpiking']:\n",
        "    for video in val_df[val_df['label'] == class_name]['clip_path']:\n",
        "        video_filename = video.split('/')[-1].replace('.avi', '')\n",
        "        # print(f\"Extracting and augmenting frames for {class_name} - {video_filename}\")\n",
        "        extract_frames('/content/ucf101-action-recognition' + video, class_name, video_filename, frame_save_path)"
      ],
      "metadata": {
        "id": "Rln6K1ZgX9ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Dataset loader\n",
        "class VideoFrameDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, input_frames=10, target_frames=5):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.input_frames = input_frames\n",
        "        self.target_frames = target_frames\n",
        "        self.data = []\n",
        "\n",
        "        for class_dir in os.listdir(root_dir):\n",
        "            class_path = os.path.join(root_dir, class_dir)\n",
        "            for video_dir in os.listdir(class_path):\n",
        "                video_path = os.path.join(class_path, video_dir)\n",
        "                frames = sorted(os.listdir(video_path))\n",
        "                self.data.append((video_path, frames))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path, frames = self.data[idx]\n",
        "        frames = sorted(frames)\n",
        "\n",
        "        # Load frames as tensors\n",
        "        input_frames = []\n",
        "        target_frames = []\n",
        "        for i, frame_name in enumerate(frames):\n",
        "            frame_path = os.path.join(video_path, frame_name)\n",
        "            img = Image.open(frame_path)\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            if i < self.input_frames:\n",
        "                input_frames.append(img)\n",
        "            elif i < self.input_frames + self.target_frames:\n",
        "                target_frames.append(img)\n",
        "\n",
        "        input_frames = torch.stack(input_frames)\n",
        "        target_frames = torch.stack(target_frames)\n",
        "        return input_frames, target_frames\n",
        "\n",
        "# Define PredRNN (simplified version for frame prediction)\n",
        "class PredRNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super(PredRNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.input_dim = input_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "input_frames = 10\n",
        "target_frames = 5\n",
        "input_dim = 64 * 64  # Assuming frames are resized to 64x64\n",
        "hidden_dim = 512\n",
        "output_dim = 64 * 64\n",
        "num_layers = 2\n",
        "batch_size = 16\n",
        "epochs = 30\n",
        "lr = 0.001\n",
        "\n",
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(),  # Convert to grayscale\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load data\n",
        "train_dataset = VideoFrameDataset(root_dir=\"frames_raw\", transform=transform, input_frames=input_frames, target_frames=target_frames)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = VideoFrameDataset(root_dir=\"frames_val\", transform=transform, input_frames=input_frames, target_frames=target_frames)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, loss, optimizer\n",
        "model = PredRNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Training loop\n",
        "def train():\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for inputs, targets in tqdm(train_loader):\n",
        "            inputs = inputs.view(inputs.size(0), input_frames, -1).to(device)  # Flatten each frame\n",
        "            targets = targets.view(targets.size(0), target_frames, -1).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets[:, -1, :])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        val_loss = validate()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "# Validation loop\n",
        "def validate():\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.view(inputs.size(0), input_frames, -1).to(device)\n",
        "            targets = targets.view(targets.size(0), target_frames, -1).to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets[:, -1, :])\n",
        "            val_loss += loss.item()\n",
        "    return val_loss / len(val_loader)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n"
      ],
      "metadata": {
        "id": "qQYDj3f0YBFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model\n",
        "torch.save(model, \"predrnn_model.pth\")\n",
        "\n",
        "# Save only the state dictionary\n",
        "torch.save(model.state_dict(), \"predrnn_model_state.pth\")\n"
      ],
      "metadata": {
        "id": "tllyXO4Ua4tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the test function\n",
        "def test_model(model, test_loader, output_dir=\"predicted_frames\"):\n",
        "    model.eval()\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs = inputs.view(inputs.size(0), input_frames, -1).to(device)  # Flatten input frames\n",
        "            predictions = model(inputs)  # Predict next frames\n",
        "            predictions = predictions.view(-1, 64, 64).cpu().numpy()  # Reshape predictions to image format\n",
        "\n",
        "            # Save the predicted frames\n",
        "            for i, frame in enumerate(predictions):\n",
        "                frame_output_dir = os.path.join(output_dir, f\"batch_{batch_idx}_frame_{i}.png\")\n",
        "                plt.imsave(frame_output_dir, frame, cmap=\"gray\")\n",
        "                print(f\"Saved predicted frame: {frame_output_dir}\")\n",
        "\n",
        "# Load the trained model\n",
        "model = PredRNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
        "model.load_state_dict(torch.load(\"predrnn_model_state.pth\"))\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Test the model\n",
        "test_dataset = VideoFrameDataset(root_dir=\"frames_val\", transform=transform, input_frames=input_frames, target_frames=target_frames)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # Test one sequence at a time\n",
        "test_model(model, test_loader)\n"
      ],
      "metadata": {
        "id": "zm_8Rmj4deXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example visualization\n",
        "plt.imshow(predictions[0], cmap=\"gray\")\n",
        "plt.title(\"Predicted Frame\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "k2J7l-NJh1Ll"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}