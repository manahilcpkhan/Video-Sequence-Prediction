{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY_ILf4kXbu1",
        "outputId": "7eaabab0-40f3-4177-b3f1-44bfc1d7c65a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d matthewjansen/ucf101-action-recognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J5TS2DuXkjG",
        "outputId": "605260af-b29c-495a-a44e-9934fe045f1d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/matthewjansen/ucf101-action-recognition\n",
            "License(s): CC0-1.0\n",
            "Downloading ucf101-action-recognition.zip to /content\n",
            "100% 6.52G/6.53G [01:18<00:00, 121MB/s]\n",
            "100% 6.53G/6.53G [01:18<00:00, 88.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "# /content/ucf101-action-recognition.zip\n",
        "# Path to the zip file\n",
        "zip_file_path = '/content/ucf101-action-recognition.zip'\n",
        "extracted_path = '/content/ucf101-action-recognition'\n",
        "\n",
        "# Unzipping the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_path)\n",
        "\n",
        "# Check if extraction was successful\n",
        "extracted_files = os.listdir(extracted_path)\n",
        "print(\"Files and directories after extraction:\")\n",
        "for file in extracted_files:\n",
        "    print(file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_GohP5wXpMj",
        "outputId": "2d7b4630-3dc5-4df4-c4b2-890e4dfd6608"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files and directories after extraction:\n",
            "test.csv\n",
            "test\n",
            "train.csv\n",
            "val\n",
            "train\n",
            "val.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "extracted_path = './ucf101-action-recognition'\n",
        "\n",
        "# Path to the CSV files\n",
        "train_csv_path = os.path.join(extracted_path, 'train.csv')\n",
        "val_csv_path = os.path.join(extracted_path, 'val.csv')\n",
        "\n",
        "# Load the CSV files\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "val_df = pd.read_csv(val_csv_path)\n",
        "\n",
        "# Display the first few rows of train.csv to understand its structure\n",
        "print(\"Training Data Sample:\")\n",
        "print(train_df.head())\n",
        "\n",
        "# Filter for the selected classes\n",
        "selected_classes = ['Biking', 'SoccerPenalty', 'JumpingJack', 'BasketballDunk', 'VolleyballSpiking']\n",
        "\n",
        "train_selected = train_df[train_df['label'].isin(selected_classes)]\n",
        "val_selected = val_df[val_df['label'].isin(selected_classes)]\n",
        "\n",
        "# Display filtered train and val datasets\n",
        "print(\"\\nFiltered Training Data:\")\n",
        "print(train_selected.head())\n",
        "\n",
        "print(\"\\nFiltered Validation Data:\")\n",
        "print(val_selected.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6B3eN9wZToT",
        "outputId": "a61f295c-117e-496d-c839-8c1d6e600aa0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Sample:\n",
            "         clip_name                         clip_path  label\n",
            "0  v_Swing_g05_c02  /train/Swing/v_Swing_g05_c02.avi  Swing\n",
            "1  v_Swing_g21_c03  /train/Swing/v_Swing_g21_c03.avi  Swing\n",
            "2  v_Swing_g07_c01  /train/Swing/v_Swing_g07_c01.avi  Swing\n",
            "3  v_Swing_g24_c04  /train/Swing/v_Swing_g24_c04.avi  Swing\n",
            "4  v_Swing_g20_c03  /train/Swing/v_Swing_g20_c03.avi  Swing\n",
            "\n",
            "Filtered Training Data:\n",
            "                  clip_name                                     clip_path  \\\n",
            "1282  v_JumpingJack_g13_c03  /train/JumpingJack/v_JumpingJack_g13_c03.avi   \n",
            "1283  v_JumpingJack_g14_c03  /train/JumpingJack/v_JumpingJack_g14_c03.avi   \n",
            "1284  v_JumpingJack_g25_c02  /train/JumpingJack/v_JumpingJack_g25_c02.avi   \n",
            "1285  v_JumpingJack_g01_c04  /train/JumpingJack/v_JumpingJack_g01_c04.avi   \n",
            "1286  v_JumpingJack_g06_c06  /train/JumpingJack/v_JumpingJack_g06_c06.avi   \n",
            "\n",
            "            label  \n",
            "1282  JumpingJack  \n",
            "1283  JumpingJack  \n",
            "1284  JumpingJack  \n",
            "1285  JumpingJack  \n",
            "1286  JumpingJack  \n",
            "\n",
            "Filtered Validation Data:\n",
            "                 clip_name                                   clip_path  \\\n",
            "215  v_JumpingJack_g14_c01  /val/JumpingJack/v_JumpingJack_g14_c01.avi   \n",
            "216  v_JumpingJack_g21_c03  /val/JumpingJack/v_JumpingJack_g21_c03.avi   \n",
            "217  v_JumpingJack_g05_c02  /val/JumpingJack/v_JumpingJack_g05_c02.avi   \n",
            "218  v_JumpingJack_g05_c03  /val/JumpingJack/v_JumpingJack_g05_c03.avi   \n",
            "219  v_JumpingJack_g11_c01  /val/JumpingJack/v_JumpingJack_g11_c01.avi   \n",
            "\n",
            "           label  \n",
            "215  JumpingJack  \n",
            "216  JumpingJack  \n",
            "217  JumpingJack  \n",
            "218  JumpingJack  \n",
            "219  JumpingJack  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Path for saving frames\n",
        "frame_save_path = '/content/frames_raw/'\n",
        "\n",
        "# Ensure the directory for saving frames exists\n",
        "os.makedirs(frame_save_path, exist_ok=True)\n",
        "\n",
        "# Function to extract frames from a video\n",
        "def extract_frames(video_path, class_name, clip_name, save_path, frame_size=(64, 64)):\n",
        "    # Create a directory for the class if it doesn't exist\n",
        "    class_save_path = os.path.join(save_path, class_name)\n",
        "    os.makedirs(class_save_path, exist_ok=True)\n",
        "\n",
        "    # Create a directory for the specific video clip\n",
        "    clip_save_path = os.path.join(class_save_path, clip_name)\n",
        "    os.makedirs(clip_save_path, exist_ok=True)\n",
        "\n",
        "    # Open video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Read frames and save them\n",
        "    frame_num = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Resize the frame\n",
        "        frame_resized = cv2.resize(frame, frame_size)\n",
        "\n",
        "        # Convert to grayscale (optional)\n",
        "        frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Save frame as an image\n",
        "        frame_filename = f\"{clip_name}_frame_{frame_num:03d}.jpg\"\n",
        "        cv2.imwrite(os.path.join(clip_save_path, frame_filename), frame_gray)\n",
        "\n",
        "        frame_num += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "# Example: Extract frames from videos for the 'JumpingJack' class\n",
        "# for class_name in ['Biking', 'SoccerPenalty', 'JumpingJack', 'BasketballDunk', 'VolleyballSpiking']:\n",
        "#     for video in train_df[train_df['label'] == class_name]['clip_path']:\n",
        "#         video_filename = video#.split('/')[-1]#.replace('.avi', '')\n",
        "#         print(f\"Extracting frames for {class_name} - {video_filename}\")\n",
        "#         extract_frames('./ucf101-action-recognition'+video, class_name, video_filename, frame_save_path)\n",
        "for class_name in ['Biking', 'SoccerPenalty', 'JumpingJack', 'BasketballDunk', 'VolleyballSpiking']:\n",
        "    for video in train_df[train_df['label'] == class_name]['clip_path']:\n",
        "        video_filename = video.split('/')[-1].replace('.avi', '')\n",
        "        # print(f\"Extracting and augmenting frames for {class_name} - {video_filename}\")\n",
        "        extract_frames('/content/ucf101-action-recognition' + video, class_name, video_filename, frame_save_path)"
      ],
      "metadata": {
        "id": "skM7l0vaX26V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame_save_path = '/content/frames_val/'\n",
        "\n",
        "for class_name in ['Biking', 'SoccerPenalty', 'JumpingJack', 'BasketballDunk', 'VolleyballSpiking']:\n",
        "    for video in val_df[val_df['label'] == class_name]['clip_path']:\n",
        "        video_filename = video.split('/')[-1].replace('.avi', '')\n",
        "        # print(f\"Extracting and augmenting frames for {class_name} - {video_filename}\")\n",
        "        extract_frames('/content/ucf101-action-recognition' + video, class_name, video_filename, frame_save_path)"
      ],
      "metadata": {
        "id": "Rln6K1ZgX9ij"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Dataset loader\n",
        "class VideoFrameDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, input_frames=10, target_frames=5):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.input_frames = input_frames\n",
        "        self.target_frames = target_frames\n",
        "        self.data = []\n",
        "\n",
        "        for class_dir in os.listdir(root_dir):\n",
        "            class_path = os.path.join(root_dir, class_dir)\n",
        "            for video_dir in os.listdir(class_path):\n",
        "                video_path = os.path.join(class_path, video_dir)\n",
        "                frames = sorted(os.listdir(video_path))\n",
        "                self.data.append((video_path, frames))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path, frames = self.data[idx]\n",
        "        frames = sorted(frames)\n",
        "\n",
        "        # Load frames as tensors\n",
        "        input_frames = []\n",
        "        target_frames = []\n",
        "        for i, frame_name in enumerate(frames):\n",
        "            frame_path = os.path.join(video_path, frame_name)\n",
        "            img = Image.open(frame_path)\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            if i < self.input_frames:\n",
        "                input_frames.append(img)\n",
        "            elif i < self.input_frames + self.target_frames:\n",
        "                target_frames.append(img)\n",
        "\n",
        "        input_frames = torch.stack(input_frames)\n",
        "        target_frames = torch.stack(target_frames)\n",
        "        return input_frames, target_frames\n",
        "\n",
        "# Define PredRNN (simplified version for frame prediction)\n",
        "class PredRNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super(PredRNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.input_dim = input_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "input_frames = 10\n",
        "target_frames = 5\n",
        "input_dim = 64 * 64  # Assuming frames are resized to 64x64\n",
        "hidden_dim = 512\n",
        "output_dim = 64 * 64\n",
        "num_layers = 2\n",
        "batch_size = 16\n",
        "epochs = 30\n",
        "lr = 0.001\n",
        "\n",
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(),  # Convert to grayscale\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load data\n",
        "train_dataset = VideoFrameDataset(root_dir=\"frames_raw\", transform=transform, input_frames=input_frames, target_frames=target_frames)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = VideoFrameDataset(root_dir=\"frames_val\", transform=transform, input_frames=input_frames, target_frames=target_frames)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, loss, optimizer\n",
        "model = PredRNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Training loop\n",
        "def train():\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for inputs, targets in tqdm(train_loader):\n",
        "            inputs = inputs.view(inputs.size(0), input_frames, -1).to(device)  # Flatten each frame\n",
        "            targets = targets.view(targets.size(0), target_frames, -1).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets[:, -1, :])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        val_loss = validate()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "# Validation loop\n",
        "def validate():\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.view(inputs.size(0), input_frames, -1).to(device)\n",
        "            targets = targets.view(targets.size(0), target_frames, -1).to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets[:, -1, :])\n",
        "            val_loss += loss.item()\n",
        "    return val_loss / len(val_loader)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQYDj3f0YBFS",
        "outputId": "2e78d48f-5058-4f97-f93c-69c635fc7dfa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:26<00:00,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Train Loss: 0.0750, Val Loss: 0.0548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/30, Train Loss: 0.0461, Val Loss: 0.0429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/30, Train Loss: 0.0414, Val Loss: 0.0394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/30, Train Loss: 0.0379, Val Loss: 0.0361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/30, Train Loss: 0.0346, Val Loss: 0.0342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:22<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/30, Train Loss: 0.0325, Val Loss: 0.0328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/30, Train Loss: 0.0320, Val Loss: 0.0322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:22<00:00,  1.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/30, Train Loss: 0.0302, Val Loss: 0.0326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:22<00:00,  1.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/30, Train Loss: 0.0294, Val Loss: 0.0317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:22<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/30, Train Loss: 0.0285, Val Loss: 0.0319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/30, Train Loss: 0.0274, Val Loss: 0.0308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/30, Train Loss: 0.0269, Val Loss: 0.0299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/30, Train Loss: 0.0259, Val Loss: 0.0292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/30, Train Loss: 0.0248, Val Loss: 0.0289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/30, Train Loss: 0.0241, Val Loss: 0.0278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:22<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/30, Train Loss: 0.0242, Val Loss: 0.0274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:22<00:00,  1.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/30, Train Loss: 0.0230, Val Loss: 0.0272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/30, Train Loss: 0.0226, Val Loss: 0.0264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/30, Train Loss: 0.0223, Val Loss: 0.0265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/30, Train Loss: 0.0218, Val Loss: 0.0261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/30, Train Loss: 0.0211, Val Loss: 0.0260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/30, Train Loss: 0.0208, Val Loss: 0.0252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/30, Train Loss: 0.0205, Val Loss: 0.0250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/30, Train Loss: 0.0203, Val Loss: 0.0253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:22<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/30, Train Loss: 0.0196, Val Loss: 0.0244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/30, Train Loss: 0.0192, Val Loss: 0.0247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/30, Train Loss: 0.0193, Val Loss: 0.0254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/30, Train Loss: 0.0194, Val Loss: 0.0241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/30, Train Loss: 0.0185, Val Loss: 0.0236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:23<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/30, Train Loss: 0.0180, Val Loss: 0.0236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model\n",
        "torch.save(model, \"predrnn_model.pth\")\n",
        "\n",
        "# Save only the state dictionary\n",
        "torch.save(model.state_dict(), \"predrnn_model_state.pth\")\n"
      ],
      "metadata": {
        "id": "tllyXO4Ua4tn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the test function for sequential predictions\n",
        "def test_model_sequential(model, test_loader, output_dir=\"predicted_sequence\", num_predicted_frames=10):\n",
        "    model.eval()\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, _) in enumerate(test_loader):\n",
        "            # Get the first batch (input sequence)\n",
        "            inputs = inputs.view(inputs.size(0), input_frames, -1).to(device)  # Flatten input frames\n",
        "\n",
        "            # Start with the input sequence\n",
        "            current_sequence = inputs[0]  # Take the first sequence in the batch\n",
        "            predicted_frames = []\n",
        "\n",
        "            # Generate sequential predictions\n",
        "            for _ in range(num_predicted_frames):\n",
        "                prediction = model(current_sequence.unsqueeze(0))  # Predict the next frame\n",
        "                predicted_frames.append(prediction.cpu().view(64, 64).numpy())  # Save the prediction\n",
        "\n",
        "                # Update the sequence for the next prediction\n",
        "                current_sequence = torch.cat((current_sequence[1:], prediction), dim=0)\n",
        "\n",
        "            # Save the predicted frames as a sequence\n",
        "            for i, frame in enumerate(predicted_frames):\n",
        "                frame_output_path = os.path.join(output_dir, f\"batch_{batch_idx}_pred_frame_{i}.png\")\n",
        "                plt.imsave(frame_output_path, frame, cmap=\"gray\")\n",
        "                print(f\"Saved predicted frame: {frame_output_path}\")\n",
        "\n",
        "            # Stop after the first sequence (remove this to test all sequences)\n",
        "            break\n",
        "\n",
        "# Load the trained model\n",
        "model = PredRNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
        "model.load_state_dict(torch.load(\"predrnn_model_state.pth\"))\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Test the model for a single sequence\n",
        "test_dataset = VideoFrameDataset(root_dir=\"frames_val\", transform=transform, input_frames=input_frames, target_frames=target_frames)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # Test one sequence at a time\n",
        "test_model_sequential(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXpWt2M_76lW",
        "outputId": "d739cc21-d7cc-4080-8b4c-42fe01577ddc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "Saved predicted frame: predicted_sequence/batch_0_pred_frame_0.png\n",
            "Saved predicted frame: predicted_sequence/batch_0_pred_frame_1.png\n",
            "Saved predicted frame: predicted_sequence/batch_0_pred_frame_2.png\n",
            "Saved predicted frame: predicted_sequence/batch_0_pred_frame_3.png\n",
            "Saved predicted frame: predicted_sequence/batch_0_pred_frame_4.png\n",
            "Saved predicted frame: predicted_sequence/batch_0_pred_frame_5.png\n",
            "Saved predicted frame: predicted_sequence/batch_0_pred_frame_6.png\n",
            "Saved predicted frame: predicted_sequence/batch_0_pred_frame_7.png\n",
            "Saved predicted frame: predicted_sequence/batch_0_pred_frame_8.png\n",
            "Saved predicted frame: predicted_sequence/batch_0_pred_frame_9.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-a26aba79447c>:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"predrnn_model_state.pth\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import os\n",
        "# from torchvision import transforms\n",
        "# from PIL import Image\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Define the test function\n",
        "# def test_model(model, test_loader, output_dir=\"predicted_frames\"):\n",
        "#     model.eval()\n",
        "#     os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "#             inputs = inputs.view(inputs.size(0), input_frames, -1).to(device)  # Flatten input frames\n",
        "#             predictions = model(inputs)  # Predict next frames\n",
        "#             predictions = predictions.view(-1, 64, 64).cpu().numpy()  # Reshape predictions to image format\n",
        "\n",
        "#             # Save the predicted frames\n",
        "#             for i, frame in enumerate(predictions):\n",
        "#                 frame_output_dir = os.path.join(output_dir, f\"batch_{batch_idx}_frame_{i}.png\")\n",
        "#                 plt.imsave(frame_output_dir, frame, cmap=\"gray\")\n",
        "#                 print(f\"Saved predicted frame: {frame_output_dir}\")\n",
        "\n",
        "# # Load the trained model\n",
        "# model = PredRNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
        "# model.load_state_dict(torch.load(\"predrnn_model_state.pth\"))\n",
        "# print(\"Model loaded successfully.\")\n",
        "\n",
        "# # Test the model\n",
        "# test_dataset = VideoFrameDataset(root_dir=\"frames_val\", transform=transform, input_frames=input_frames, target_frames=target_frames)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # Test one sequence at a time\n",
        "# test_model(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zm_8Rmj4deXG",
        "outputId": "4cae73e5-16c4-4495-91a3-025b42d35a1b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-1a2225caf26f>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"predrnn_model_state.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "Saved predicted frame: predicted_frames/batch_0_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_1_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_2_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_3_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_4_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_5_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_6_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_7_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_8_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_9_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_10_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_11_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_12_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_13_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_14_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_15_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_16_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_17_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_18_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_19_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_20_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_21_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_22_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_23_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_24_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_25_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_26_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_27_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_28_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_29_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_30_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_31_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_32_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_33_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_34_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_35_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_36_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_37_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_38_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_39_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_40_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_41_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_42_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_43_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_44_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_45_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_46_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_47_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_48_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_49_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_50_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_51_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_52_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_53_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_54_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_55_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_56_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_57_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_58_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_59_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_60_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_61_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_62_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_63_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_64_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_65_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_66_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_67_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_68_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_69_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_70_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_71_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_72_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_73_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_74_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_75_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_76_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_77_frame_0.png\n",
            "Saved predicted frame: predicted_frames/batch_78_frame_0.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Dataset loader\n",
        "class VideoFrameDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, input_frames=10, target_frames=5):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.input_frames = input_frames\n",
        "        self.target_frames = target_frames\n",
        "        self.data = []\n",
        "\n",
        "        for class_dir in os.listdir(root_dir):\n",
        "            class_path = os.path.join(root_dir, class_dir)\n",
        "            for video_dir in os.listdir(class_path):\n",
        "                video_path = os.path.join(class_path, video_dir)\n",
        "                frames = sorted(os.listdir(video_path))\n",
        "                self.data.append((video_path, frames))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path, frames = self.data[idx]\n",
        "        frames = sorted(frames)\n",
        "\n",
        "        # Load frames as tensors\n",
        "        input_frames = []\n",
        "        target_frames = []\n",
        "        for i, frame_name in enumerate(frames):\n",
        "            frame_path = os.path.join(video_path, frame_name)\n",
        "            img = Image.open(frame_path)\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            if i < self.input_frames:\n",
        "                input_frames.append(img)\n",
        "            elif i < self.input_frames + self.target_frames:\n",
        "                target_frames.append(img)\n",
        "\n",
        "        input_frames = torch.stack(input_frames)\n",
        "        target_frames = torch.stack(target_frames)\n",
        "        return input_frames, target_frames\n",
        "\n",
        "# Define PredRNN (simplified version for frame prediction)\n",
        "class PredRNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super(PredRNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.input_dim = input_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Function to load frames from a directory\n",
        "def load_frames_from_directory(directory, transform, num_frames):\n",
        "    frames = sorted(os.listdir(directory))[:num_frames]\n",
        "    images = []\n",
        "    for frame in frames:\n",
        "        frame_path = os.path.join(directory, frame)\n",
        "        img = Image.open(frame_path)\n",
        "        if transform:\n",
        "            img = transform(img)\n",
        "        images.append(img)\n",
        "    return torch.stack(images)\n",
        "\n",
        "# Define the test function with comparison\n",
        "def test_model_with_comparison(model, input_dir, target_dir, output_dir=\"predicted_vs_actual\", num_predicted_frames=10):\n",
        "    model.eval()\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load the input frames\n",
        "    input_frames = load_frames_from_directory(input_dir, transform, num_frames=input_frames).to(device)\n",
        "    current_sequence = input_frames  # Start with input frames\n",
        "\n",
        "    # Predict the next frames\n",
        "    predicted_frames = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_predicted_frames):\n",
        "            prediction = model(current_sequence.unsqueeze(0))  # Predict the next frame\n",
        "            predicted_frames.append(prediction.cpu().view(64, 64).numpy())  # Save the prediction\n",
        "\n",
        "            # Update the sequence for the next prediction\n",
        "            current_sequence = torch.cat((current_sequence[1:], prediction), dim=0)\n",
        "\n",
        "    # Load the actual target frames\n",
        "    actual_frames = load_frames_from_directory(target_dir, transform, num_frames=num_predicted_frames)\n",
        "\n",
        "    # Compare and save the frames\n",
        "    for i, (pred_frame, actual_frame) in enumerate(zip(predicted_frames, actual_frames)):\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
        "        axs[0].imshow(pred_frame, cmap=\"gray\")\n",
        "        axs[0].set_title(\"Predicted Frame\")\n",
        "        axs[0].axis(\"off\")\n",
        "        axs[1].imshow(actual_frame[0].cpu().numpy(), cmap=\"gray\")\n",
        "        axs[1].set_title(\"Actual Frame\")\n",
        "        axs[1].axis(\"off\")\n",
        "\n",
        "        output_path = os.path.join(output_dir, f\"comparison_frame_{i}.png\")\n",
        "        plt.savefig(output_path)\n",
        "        plt.close(fig)\n",
        "        print(f\"Saved comparison: {output_path}\")\n",
        "\n",
        "# Load the trained model\n",
        "model = PredRNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
        "model.load_state_dict(torch.load(\"predrnn_model_state.pth\"))\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Specify input and target directories\n",
        "input_dir = \"/content/frames_val/Biking/v_Biking_g02_c05\"  # Path to directory with 10 input frames\n",
        "target_dir = \"/content/frames_val/Biking/v_Biking_g02_c05\"  # Path to directory with target frames for comparison\n",
        "\n",
        "# Test the model with comparison\n",
        "test_model_with_comparison(model, input_dir, target_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "GHqAcnkO6ttJ",
        "outputId": "4d82feb1-e90d-4c0f-9d04-ef1c65edb666"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-49f5efd99174>:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"predrnn_model_state.pth\"))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'predrnn_model_state.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-49f5efd99174>\u001b[0m in \u001b[0;36m<cell line: 128>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;31m# Load the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predrnn_model_state.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model loaded successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'predrnn_model_state.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "# Directory containing the predicted frames\n",
        "frames_directory = '/content/predicted_sequence'\n",
        "\n",
        "# List all PNG files in the directory\n",
        "png_files = sorted([file for file in os.listdir(frames_directory) if file.endswith('.png')])\n",
        "\n",
        "# Check if there are any PNG files\n",
        "if not png_files:\n",
        "    print(\"No PNG files found in the directory.\")\n",
        "else:\n",
        "    # Get the size of the first image to determine video dimensions\n",
        "    first_image_path = os.path.join(frames_directory, png_files[0])\n",
        "    first_image = cv2.imread(first_image_path)\n",
        "    height, width, layers = first_image.shape\n",
        "    size = (width, height)\n",
        "\n",
        "    # Define the codec and create a VideoWriter object\n",
        "    video_path = 'predicted_frames_video.avi'\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Codec for AVI files\n",
        "    video = cv2.VideoWriter(video_path, fourcc, 30, size)\n",
        "\n",
        "    # Read each PNG file and write it to the video\n",
        "    for file in png_files:\n",
        "        image_path = os.path.join(frames_directory, file)\n",
        "        frame = cv2.imread(image_path)\n",
        "        video.write(frame)\n",
        "\n",
        "    # Release the video writer\n",
        "    video.release()\n",
        "\n",
        "    print(f\"Video created successfully: {video_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6009OKo7Fb8",
        "outputId": "aa765995-9d8f-4885-8dfd-bf493f3395c9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video created successfully: predicted_frames_video.avi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# class TransformerFramePredictor(nn.Module):\n",
        "#     def __init__(self, input_dim, num_heads, num_layers, hidden_dim, seq_length):\n",
        "#         super(TransformerFramePredictor, self).__init__()\n",
        "#         self.input_dim = input_dim\n",
        "#         self.seq_length = seq_length\n",
        "#         self.embedding = nn.Linear(input_dim, hidden_dim)\n",
        "#         self.positional_encoding = nn.Parameter(torch.zeros(1, seq_length, hidden_dim))\n",
        "#         self.transformer = nn.Transformer(\n",
        "#             d_model=hidden_dim, nhead=num_heads, num_encoder_layers=num_layers, num_decoder_layers=num_layers\n",
        "#         )\n",
        "#         self.fc = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "#     def forward(self, src, tgt):\n",
        "#         # src: (seq_length, batch_size, input_dim)\n",
        "#         # tgt: (seq_length, batch_size, input_dim)\n",
        "#         src = self.embedding(src) + self.positional_encoding\n",
        "#         tgt = self.embedding(tgt) + self.positional_encoding\n",
        "#         output = self.transformer(src, tgt)\n",
        "#         output = self.fc(output)\n",
        "#         return output\n",
        "class TransformerFramePredictor(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads, num_layers, hidden_dim, max_seq_length):\n",
        "        super(TransformerFramePredictor, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
        "        self.register_buffer(\"positional_encoding\", self._generate_positional_encoding(max_seq_length, hidden_dim))\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=hidden_dim, nhead=num_heads, num_encoder_layers=num_layers, num_decoder_layers=num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def _generate_positional_encoding(self, max_seq_length, hidden_dim):\n",
        "        # Generate positional encoding (max_seq_length, hidden_dim)\n",
        "        position = torch.arange(max_seq_length).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, hidden_dim, 2) * (-torch.log(torch.tensor(10000.0)) / hidden_dim))\n",
        "        pe = torch.zeros(max_seq_length, hidden_dim)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        return pe.unsqueeze(1)  # Shape: (max_seq_length, 1, hidden_dim)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # src: (seq_length, batch_size, input_dim)\n",
        "        # tgt: (seq_length, batch_size, input_dim)\n",
        "        src = self.embedding(src)\n",
        "        tgt = self.embedding(tgt)\n",
        "\n",
        "        # Ensure positional encoding matches the actual sequence length\n",
        "        seq_length_src = src.size(0)\n",
        "        seq_length_tgt = tgt.size(0)\n",
        "\n",
        "        src_pos_enc = self.positional_encoding[:seq_length_src, :, :].to(src.device)\n",
        "        tgt_pos_enc = self.positional_encoding[:seq_length_tgt, :, :].to(tgt.device)\n",
        "\n",
        "        src = src + src_pos_enc\n",
        "        tgt = tgt + tgt_pos_enc\n",
        "\n",
        "        output = self.transformer(src, tgt)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "def load_frames_from_directory(directory, transform, num_frames):\n",
        "    frames = sorted(os.listdir(directory))[:num_frames]\n",
        "    images = []\n",
        "    for frame in frames:\n",
        "        frame_path = os.path.join(directory, frame)\n",
        "        img = Image.open(frame_path)\n",
        "        if transform:\n",
        "            img = transform(img)\n",
        "        images.append(img)\n",
        "    return torch.stack(images)"
      ],
      "metadata": {
        "id": "XVTgFegE7KFm"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Training function\n",
        "def train_transformer_model(model, train_loader, val_loader, epochs, lr):\n",
        "    model.train()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0.0\n",
        "        model.train()\n",
        "        for inputs, targets in tqdm(train_loader):\n",
        "            inputs = inputs.view(inputs.size(0), input_frames, -1).to(device)  # Flatten input frames\n",
        "            targets = targets.view(targets.size(0), target_frames, -1).to(device)  # Flatten target frames\n",
        "\n",
        "            src = inputs.permute(1, 0, 2)  # (seq_length, batch_size, input_dim)\n",
        "            tgt = targets.permute(1, 0, 2)  # (seq_length, batch_size, input_dim)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(src, tgt[:-1])\n",
        "            loss = criterion(predictions, tgt[1:])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        val_loss = validate_transformer_model(model, val_loader, criterion)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss / len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save the best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), \"transformer_model_state.pth\")\n",
        "            print(\"Saved best model.\")\n",
        "\n",
        "# Validation function\n",
        "def validate_transformer_model(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.view(inputs.size(0), input_frames, -1).to(device)  # Flatten input frames\n",
        "            targets = targets.view(targets.size(0), target_frames, -1).to(device)  # Flatten target frames\n",
        "\n",
        "            src = inputs.permute(1, 0, 2)  # (seq_length, batch_size, input_dim)\n",
        "            tgt = targets.permute(1, 0, 2)  # (seq_length, batch_size, input_dim)\n",
        "\n",
        "            predictions = model(src, tgt[:-1])\n",
        "            loss = criterion(predictions, tgt[1:])\n",
        "            val_loss += loss.item()\n",
        "    return val_loss / len(val_loader)\n"
      ],
      "metadata": {
        "id": "Bvt8spvoYkBs"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "class VideoFrameDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, input_frames=10, target_frames=5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Directory with all the video frames organized by classes and videos.\n",
        "            transform (callable, optional): A function/transform to apply to the frames.\n",
        "            input_frames (int): Number of input frames for the model.\n",
        "            target_frames (int): Number of target frames to predict.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.input_frames = input_frames\n",
        "        self.target_frames = target_frames\n",
        "        self.data = []\n",
        "\n",
        "        # Collect all videos\n",
        "        for class_dir in os.listdir(root_dir):\n",
        "            class_path = os.path.join(root_dir, class_dir)\n",
        "            for video_dir in os.listdir(class_path):\n",
        "                video_path = os.path.join(class_path, video_dir)\n",
        "                frames = sorted(os.listdir(video_path))\n",
        "                # Ensure sufficient frames for input and target\n",
        "                if len(frames) >= input_frames + target_frames:\n",
        "                    self.data.append((video_path, frames))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path, frames = self.data[idx]\n",
        "\n",
        "        # Select input and target frames\n",
        "        input_frames = frames[:self.input_frames]\n",
        "        target_frames = frames[self.input_frames:self.input_frames + self.target_frames]\n",
        "\n",
        "        # Load frames\n",
        "        input_tensors = []\n",
        "        target_tensors = []\n",
        "\n",
        "        for frame_name in input_frames:\n",
        "            frame_path = os.path.join(video_path, frame_name)\n",
        "            img = Image.open(frame_path)\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            input_tensors.append(img)\n",
        "\n",
        "        for frame_name in target_frames:\n",
        "            frame_path = os.path.join(video_path, frame_name)\n",
        "            img = Image.open(frame_path)\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            target_tensors.append(img)\n",
        "\n",
        "        # Stack tensors\n",
        "        input_tensor = torch.stack(input_tensors)  # Shape: (input_frames, channels, height, width)\n",
        "        target_tensor = torch.stack(target_tensors)  # Shape: (target_frames, channels, height, width)\n",
        "\n",
        "        return input_tensor, target_tensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(),  # Convert frames to grayscale (optional)\n",
        "    transforms.Resize((64, 64)),  # Resize frames to 64x64 pixels\n",
        "    transforms.ToTensor(),  # Convert frames to PyTorch tensors\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize pixel values to [-1, 1]\n",
        "])\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Parameters\n",
        "batch_size = 16\n",
        "input_frames = 10\n",
        "target_frames = 10\n",
        "\n",
        "# Training dataset and DataLoader\n",
        "train_dataset = VideoFrameDataset(\n",
        "    root_dir=\"frames_raw\",\n",
        "    transform=transform,\n",
        "    input_frames=input_frames,\n",
        "    target_frames=target_frames\n",
        ")\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Validation dataset and DataLoader\n",
        "val_dataset = VideoFrameDataset(\n",
        "    root_dir=\"frames_val\",\n",
        "    transform=transform,\n",
        "    input_frames=input_frames,\n",
        "    target_frames=target_frames\n",
        ")\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# Define PredRNN (simplified version for frame prediction)\n",
        "\n",
        "# Hyperparameters\n",
        "input_dim = 64 * 64  # Frame size after flattening\n",
        "hidden_dim = 512\n",
        "num_heads = 8\n",
        "num_layers = 4\n",
        "seq_length = 10\n",
        "batch_size = 16\n",
        "epochs = 20\n",
        "lr = 0.001\n",
        "\n",
        "# Initialize the Transformer model\n",
        "# Initialize the Transformer model\n",
        "model = TransformerFramePredictor(\n",
        "    input_dim=64 * 64,  # Flattened input size\n",
        "    num_heads=8,\n",
        "    num_layers=4,\n",
        "    hidden_dim=512,\n",
        "    max_seq_length=input_frames + target_frames  # Max sequence length\n",
        ").to(device)\n",
        "\n",
        "# Train the model\n",
        "train_transformer_model(model, train_loader, val_loader, epochs, lr)\n",
        "\n",
        "\n",
        "# # Validation loop\n",
        "# def validate():\n",
        "#     model.eval()\n",
        "#     val_loss = 0.0\n",
        "#     with torch.no_grad():\n",
        "#         for inputs, targets in val_loader:\n",
        "#             inputs = inputs.view(inputs.size(0), input_frames, -1).to(device)\n",
        "#             targets = targets.view(targets.size(0), target_frames, -1).to(device)\n",
        "\n",
        "#             outputs = model(inputs)\n",
        "#             loss = criterion(outputs, targets[:, -1, :])\n",
        "#             val_loss += loss.item()\n",
        "#     return val_loss / len(val_loader)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     train()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAYS8VeZYmui",
        "outputId": "81470334-5995-435e-ca22-91b622526e94"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:32<00:00,  1.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 0.2957, Val Loss: 0.2197\n",
            "Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:32<00:00,  1.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20, Train Loss: 0.2443, Val Loss: 0.2253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:32<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20, Train Loss: 0.2215, Val Loss: 0.2021\n",
            "Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:34<00:00,  1.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20, Train Loss: 0.2188, Val Loss: 0.1995\n",
            "Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:35<00:00,  1.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20, Train Loss: 0.2166, Val Loss: 0.2044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:34<00:00,  1.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20, Train Loss: 0.2282, Val Loss: 0.2335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:35<00:00,  1.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20, Train Loss: 0.2332, Val Loss: 0.2250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:35<00:00,  1.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20, Train Loss: 0.2286, Val Loss: 0.2246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:36<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20, Train Loss: 0.2228, Val Loss: 0.2153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:36<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20, Train Loss: 0.2234, Val Loss: 0.2118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:36<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20, Train Loss: 0.2199, Val Loss: 0.2086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:36<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20, Train Loss: 0.2130, Val Loss: 0.2143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:36<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20, Train Loss: 0.2106, Val Loss: 0.2104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:36<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20, Train Loss: 0.2106, Val Loss: 0.2088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:36<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20, Train Loss: 0.2092, Val Loss: 0.2134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:36<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20, Train Loss: 0.2083, Val Loss: 0.2102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:36<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20, Train Loss: 0.2086, Val Loss: 0.2101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:36<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20, Train Loss: 0.2074, Val Loss: 0.2107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:36<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20, Train Loss: 0.2083, Val Loss: 0.2075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:35<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20, Train Loss: 0.2076, Val Loss: 0.2092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model\n",
        "torch.save(model, \"transformer_model.pth\")\n",
        "print(\"Model saved as transformer_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUJgwK3wz-JW",
        "outputId": "a9e2461a-08e7-4af8-ab62-90282a56adf8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as transformer_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save only the model's state dictionary\n",
        "torch.save(model.state_dict(), \"transformer_model_state.pth\")\n",
        "print(\"Model state dictionary saved as transformer_model_state.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-84l3l40K4-",
        "outputId": "9ba87eca-c268-4c19-c197-7c571bebf35f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model state dictionary saved as transformer_model_state.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the entire model\n",
        "model = torch.load(\"transformer_model.pth\")\n",
        "model.eval()\n",
        "print(\"Transformer model loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "teXvk4Jf0SmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recreate the model structure\n",
        "model = TransformerFramePredictor(\n",
        "    input_dim=64 * 64,\n",
        "    num_heads=8,\n",
        "    num_layers=4,\n",
        "    hidden_dim=512,\n",
        "    max_seq_length=input_frames + target_frames\n",
        ").to(device)\n",
        "\n",
        "# Load the state dictionary\n",
        "model.load_state_dict(torch.load(\"transformer_model_state.pth\"))\n",
        "model.eval()\n",
        "print(\"Transformer model state dictionary loaded successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76oqZm8H0ThS",
        "outputId": "f10b542e-843f-4db3-f50b-d35e18e00887"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer model state dictionary loaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-a0c2d888c1eb>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"transformer_model_state.pth\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to test the Transformer model\n",
        "def test_transformer_model(model, input_dir, target_dir, output_dir=\"transformer_predicted_vs_actual\", num_predicted_frames=10):\n",
        "    model.eval()\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load the input frames\n",
        "    input_frames = load_frames_from_directory(input_dir, transform, num_frames=10).to(device)\n",
        "    input_frames_flattened = input_frames.view(input_frames.size(0), -1)  # Flatten input frames\n",
        "    input_frames_flattened = input_frames_flattened.unsqueeze(1)  # Add batch dimension\n",
        "\n",
        "    # Start prediction\n",
        "    predicted_frames = []\n",
        "    with torch.no_grad():\n",
        "        # Prepare the initial source and target\n",
        "        src = input_frames_flattened  # Source sequence (input frames)\n",
        "        tgt = src[-1:].repeat(num_predicted_frames, 1, 1)  # Start with the last frame as the target\n",
        "\n",
        "        for i in range(num_predicted_frames):\n",
        "            prediction = model(src, tgt)  # Predict the next frame\n",
        "            predicted_frame = prediction[-1].cpu().view(64, 64).numpy()  # Extract the last frame\n",
        "            predicted_frames.append(predicted_frame)\n",
        "\n",
        "            # Update the source for the next iteration\n",
        "            tgt = torch.cat((tgt[1:], prediction[-1:]), dim=0)\n",
        "\n",
        "    # Load the actual target frames\n",
        "    actual_frames = load_frames_from_directory(target_dir, transform, num_frames=num_predicted_frames)\n",
        "\n",
        "    # Compare and save the frames\n",
        "    for i, (pred_frame, actual_frame) in enumerate(zip(predicted_frames, actual_frames)):\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
        "        axs[0].imshow(pred_frame, cmap=\"gray\")\n",
        "        axs[0].set_title(\"Predicted Frame\")\n",
        "        axs[0].axis(\"off\")\n",
        "        axs[1].imshow(actual_frame[0].cpu().numpy(), cmap=\"gray\")\n",
        "        axs[1].set_title(\"Actual Frame\")\n",
        "        axs[1].axis(\"off\")\n",
        "\n",
        "        output_path = os.path.join(output_dir, f\"comparison_frame_{i}.png\")\n",
        "        plt.savefig(output_path)\n",
        "        plt.close(fig)\n",
        "        print(f\"Saved comparison: {output_path}\")\n",
        "\n",
        "# Load the trained Transformer model\n",
        "# model = TransformerFramePredictor(\n",
        "#     input_dim=64 * 64, num_heads=8, num_layers=6, hidden_dim=512, seq_length=10\n",
        "# ).to(device)\n",
        "\n",
        "# model.load_state_dict(torch.load(\"transformer_model_state.pth\"))\n",
        "print(\"Transformer model loaded successfully.\")\n",
        "\n",
        "# Specify input and target directories\n",
        "input_dir = \"/content/frames_val/BasketballDunk/v_BasketballDunk_g04_c02\"  # Path to directory with 10 input frames\n",
        "target_dir = \"/content/frames_val/BasketballDunk/v_BasketballDunk_g04_c02\"  # Path to directory with target frames for comparison\n",
        "\n",
        "# Test the model with comparison\n",
        "test_transformer_model(model, input_dir, target_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFiBLnuMW_Og",
        "outputId": "fb9dda58-54e1-42ab-d8cb-7c114e810721"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer model loaded successfully.\n",
            "Saved comparison: transformer_predicted_vs_actual/comparison_frame_0.png\n",
            "Saved comparison: transformer_predicted_vs_actual/comparison_frame_1.png\n",
            "Saved comparison: transformer_predicted_vs_actual/comparison_frame_2.png\n",
            "Saved comparison: transformer_predicted_vs_actual/comparison_frame_3.png\n",
            "Saved comparison: transformer_predicted_vs_actual/comparison_frame_4.png\n",
            "Saved comparison: transformer_predicted_vs_actual/comparison_frame_5.png\n",
            "Saved comparison: transformer_predicted_vs_actual/comparison_frame_6.png\n",
            "Saved comparison: transformer_predicted_vs_actual/comparison_frame_7.png\n",
            "Saved comparison: transformer_predicted_vs_actual/comparison_frame_8.png\n",
            "Saved comparison: transformer_predicted_vs_actual/comparison_frame_9.png\n"
          ]
        }
      ]
    }
  ]
}