{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Arfa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "C:\\Users\\Ali Arfa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Path: C:\\Users\\Ali Arfa\\Downloads\\deep_learning_project\\test\\Biking\\v_Biking_g06_c01.avi\n",
      "Resolved full path: C:\\Users\\Ali Arfa\\Downloads\\deep_learning_project\\test\\Biking\\v_Biking_g06_c01.avi\n",
      "Full Path: C:\\Users\\Ali Arfa\\Downloads\\deep_learning_project\\test\\Biking\\v_Biking_g06_c01.avi\n",
      "Label: Biking\n",
      "Full Path: C:\\Users\\Ali Arfa\\Downloads\\deep_learning_project\\test\\Biking\\v_Biking_g06_c01.avi\n",
      "Resolved full path: C:\\Users\\Ali Arfa\\Downloads\\deep_learning_project\\test\\Biking\\v_Biking_g06_c01.avi\n",
      "Full Path: C:\\Users\\Ali Arfa\\Downloads\\deep_learning_project\\test\\Biking\\v_Biking_g06_c01.avi\n",
      "Resolved full path: C:\\Users\\Ali Arfa\\Downloads\\deep_learning_project\\test\\Biking\\v_Biking_g06_c01.avi\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "the batch number of src and tgt must be equal",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 284\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# Test each model\u001b[39;00m\n\u001b[0;32m    283\u001b[0m evaluate_model_from_csv(predrnn_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredRNN\u001b[39m\u001b[38;5;124m'\u001b[39m, clip_name, output_dir)\n\u001b[1;32m--> 284\u001b[0m \u001b[43mevaluate_model_from_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransformer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m evaluate_model_from_csv(conv_lstm_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConvLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m, clip_name, output_dir)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# # Test each model\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# evaluate_model(predrnn_model, 'PredRNN', input_dir, target_dir, output_dir)\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# evaluate_model(transformer_model, 'Transformer', input_dir, target_dir, output_dir)\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# evaluate_model(conv_lstm_model, 'ConvLSTM', input_dir, target_dir, output_dir)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[34], line 187\u001b[0m, in \u001b[0;36mevaluate_model_from_csv\u001b[1;34m(model, model_name, clip_name, output_dir, num_input_frames, num_predicted_frames)\u001b[0m\n\u001b[0;32m    184\u001b[0m     pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(pred[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Transformer\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;66;03m# Ensure src and tgt batch sizes match\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43msequence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Flatten src\u001b[39;49;00m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Flatten tgt\u001b[39;49;00m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m     pred \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)  \u001b[38;5;66;03m# Reshape: [batch_size, channels, height, width]\u001b[39;00m\n\u001b[0;32m    192\u001b[0m     tgt \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Update tgt with the latest prediction\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[34], line 67\u001b[0m, in \u001b[0;36mTransformerFramePredictor.forward\u001b[1;34m(self, src, tgt)\u001b[0m\n\u001b[0;32m     65\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(src) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding[:src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), :]\u001b[38;5;241m.\u001b[39mto(src\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     66\u001b[0m tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(tgt) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding[:tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), :]\u001b[38;5;241m.\u001b[39mto(tgt\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 67\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(output)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:210\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    208\u001b[0m is_batched \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of src and tgt must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m!=\u001b[39m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of src and tgt must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: the batch number of src and tgt must be equal"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the ConvLSTM model using Keras (TensorFlow)\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def build_conv_lstm(input_shape=(10, 64, 64, 1)):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.ConvLSTM2D(filters=64, kernel_size=(3, 3), activation='relu', \n",
    "                                input_shape=input_shape, return_sequences=True, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ConvLSTM2D(filters=64, kernel_size=(3, 3), activation='relu', return_sequences=True, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv3D(filters=1, kernel_size=(3, 3, 3), activation='sigmoid', padding='same'))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# PyTorch Models: PredRNN and Transformer\n",
    "class PredRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(PredRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class TransformerFramePredictor(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, num_layers, hidden_dim, max_seq_length):\n",
    "        super(TransformerFramePredictor, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(max_seq_length, hidden_dim))\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=hidden_dim, nhead=num_heads, num_encoder_layers=num_layers, num_decoder_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src) + self.positional_encoding[:src.size(0), :].to(src.device)\n",
    "        tgt = self.embedding(tgt) + self.positional_encoding[:tgt.size(0), :].to(tgt.device)\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "# Load the trained ConvLSTM model\n",
    "conv_lstm_model = build_conv_lstm((10, 64, 64, 1))\n",
    "conv_lstm_model.load_weights('conv_lstm_model.h5')  # Update path to the saved model\n",
    "\n",
    "# Load PredRNN and Transformer models\n",
    "predrnn_model = PredRNN(input_dim=64*64, hidden_dim=512, output_dim=64*64, num_layers=2).to(device)\n",
    "predrnn_model.load_state_dict(torch.load('predrnn_model_state.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "# model = TransformerFramePredictor(\n",
    "#     input_dim=64 * 64,  # Flattened input size\n",
    "#     num_heads=8,\n",
    "#     num_layers=4,\n",
    "#     hidden_dim=512,\n",
    "#     max_seq_length=input_frames + target_frames  # Max sequence length\n",
    "# ).to(device)\n",
    "# Load the checkpoint and adjust positional encoding\n",
    "checkpoint = torch.load('transformer_model_state.pth', map_location=torch.device('cpu'))\n",
    "if 'positional_encoding' in checkpoint:\n",
    "    checkpoint['positional_encoding'] = checkpoint['positional_encoding'].squeeze(1)  # Remove the batch dimension\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "transformer_model = TransformerFramePredictor(\n",
    "    input_dim=64 * 64, num_heads=8, num_layers=4, hidden_dim=512, max_seq_length=20\n",
    ").to(device)\n",
    "transformer_model.load_state_dict(checkpoint)\n",
    "\n",
    "# transformer_model = TransformerFramePredictor(\n",
    "#     input_dim=64 * 64, num_heads=8, num_layers=4, hidden_dim=512, max_seq_length=20\n",
    "# ).to(device)\n",
    "# transformer_model.load_state_dict(torch.load('transformer_model_state.pth', map_location=torch.device('cpu')))  # Update path\n",
    "\n",
    "# Helper functions for testing\n",
    "def load_frames(directory, transform, num_frames):\n",
    "    frames = sorted(os.listdir(directory))[:num_frames]\n",
    "    images = []\n",
    "    for frame in frames:\n",
    "        img = Image.open(os.path.join(directory, frame))\n",
    "        if transform:\n",
    "            img = transform(img)\n",
    "        images.append(img)\n",
    "    return torch.stack(images)\n",
    "\n",
    "def load_frames_from_video(video_path, transform, num_frames):\n",
    "    \"\"\"\n",
    "    Load frames directly from a video file.\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        transform: Transformation pipeline for the frames.\n",
    "        num_frames (int): Number of frames to load.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened() and frame_count < num_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        img = Image.fromarray(frame)\n",
    "        if transform:\n",
    "            img = transform(img)\n",
    "        frames.append(img)\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    if len(frames) < num_frames:\n",
    "        raise ValueError(f\"Video {video_path} has fewer than {num_frames} frames.\")\n",
    "    return torch.stack(frames)\n",
    "\n",
    "# Generate video from predicted frames\n",
    "def generate_video(frames, output_path, fps=10):\n",
    "    height, width = frames[0].shape\n",
    "    video = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height), isColor=False)\n",
    "    for frame in frames:\n",
    "        frame = (frame * 255).astype(np.uint8)  # Convert normalized frame to uint8\n",
    "        video.write(cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR))  # Write frame in grayscale\n",
    "    video.release()\n",
    "    print(f\"Video saved at {output_path}\")\n",
    "\n",
    "\n",
    "# Modified testing function for CSV-based paths\n",
    "def evaluate_model_from_csv(model, model_name, clip_name, output_dir, num_input_frames=10, num_predicted_frames=5):\n",
    "    # Get input and target paths\n",
    "    input_path, label = get_clip_paths(clip_name)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(),  # Convert to grayscale\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    if input_path.endswith('.avi'):\n",
    "        # Load frames directly from the video file\n",
    "        input_frames = load_frames_from_video(input_path, transform, num_input_frames).to(device)\n",
    "        target_frames = load_frames_from_video(input_path, transform, num_predicted_frames)\n",
    "    else:\n",
    "        # Load frames from a directory\n",
    "        input_frames = load_frames(input_path, transform, num_input_frames).to(device)\n",
    "        target_frames = load_frames(input_path, transform, num_predicted_frames)\n",
    "\n",
    "\n",
    "    predictions = []\n",
    "    sequence = input_frames.unsqueeze(0)  # Add batch dimension\n",
    "    tgt = sequence[:, -1:]  # Initialize tgt with the last frame of the input sequence\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_predicted_frames):\n",
    "            if model_name == 'PredRNN':\n",
    "                # Flatten input for LSTM\n",
    "                pred = model(sequence.view(sequence.size(0), sequence.size(1), -1))\n",
    "                pred = pred.view(-1, 1, 64, 64)  # Reshape: [batch_size, channels, height, width]\n",
    "            elif model_name == 'ConvLSTM':\n",
    "                pred = conv_lstm_model.predict(sequence.cpu().numpy())\n",
    "                pred = torch.tensor(pred[0, -1]).to(device).unsqueeze(0)  # Add batch dimension\n",
    "            else:  # Transformer\n",
    "                # Ensure src and tgt batch sizes match\n",
    "                pred = model(\n",
    "                    sequence.view(sequence.size(0), sequence.size(1), -1),  # Flatten src\n",
    "                    tgt.view(sequence.size(0), tgt.size(1), -1)  # Flatten tgt\n",
    "                )\n",
    "                pred = pred.view(-1, 1, 64, 64)  # Reshape: [batch_size, channels, height, width]\n",
    "                tgt = pred.unsqueeze(1)  # Update tgt with the latest prediction\n",
    "\n",
    "            # Ensure pred has the correct shape\n",
    "            pred = pred.unsqueeze(1)  # Add sequence dimension: [batch_size, 1, channels, height, width]\n",
    "\n",
    "            predictions.append(pred.cpu().numpy().squeeze(1).squeeze(1))  # Convert to numpy, remove extra dims\n",
    "            sequence = torch.cat((sequence[:, 1:], pred), dim=1)  # Update input sequence\n",
    "\n",
    "    # Visualization and MSE calculation\n",
    "    # Visualization and MSE calculation\n",
    "    mse_scores = []\n",
    "    for i, (pred, target) in enumerate(zip(predictions, target_frames)):\n",
    "        # Reshape prediction to 2D\n",
    "        if len(pred.shape) == 4:  # Shape [batch_size, channels, height, width]\n",
    "            pred_2d = pred[0, 0]  # Take the first batch and first channel\n",
    "        elif len(pred.shape) == 3:  # Shape [channels, height, width]\n",
    "            pred_2d = pred[0]  # Take the first channel\n",
    "        elif len(pred.shape) == 2:  # Shape [height, width]\n",
    "            pred_2d = pred  # Already in the correct shape\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected pred shape: {pred.shape}\")\n",
    "\n",
    "        # Target frame\n",
    "        target_2d = target[0].cpu().numpy()  # Convert target to numpy\n",
    "\n",
    "        # Calculate MSE\n",
    "        mse = mean_squared_error(pred_2d.flatten(), target_2d.flatten())\n",
    "        mse_scores.append(mse)\n",
    "\n",
    "        # Save visualization\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(pred_2d, cmap='gray')  # Prediction\n",
    "        plt.title(f\"{model_name} Prediction\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(target_2d, cmap='gray')  # Ground truth\n",
    "        plt.title(\"Ground Truth\")\n",
    "        plt.savefig(f\"{output_dir}/{clip_name}_{model_name}_frame_{i}.png\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the test CSV\n",
    "test_csv_path = r\"C:\\Users\\Ali Arfa\\Downloads\\deep_learning_project\\test.csv\"\n",
    "test_base_folder = r\"C:/Users/Ali Arfa/Downloads/deep_learning_project\"  # e.g., 'test'\n",
    "\n",
    "test_data = pd.read_csv(test_csv_path)\n",
    "\n",
    "# Example: Selecting a specific clip\n",
    "def get_clip_paths(clip_name):\n",
    "    clip_row = test_data[test_data['clip_name'] == clip_name]\n",
    "    if clip_row.empty:\n",
    "        raise ValueError(f\"Clip {clip_name} not found in test CSV.\")\n",
    "\n",
    "    clip_path = clip_row['clip_path'].values[0]  # Path to frames or video\n",
    "    label = clip_row['label'].values[0]  # Action class\n",
    "\n",
    "    test_base_folder = r\"C:/Users/Ali Arfa/Downloads/deep_learning_project\"  # Base folder\n",
    "    # Relative path\n",
    "\n",
    "    # Ensure clip_path does not start with a slash\n",
    "    full_clip_path = os.path.normpath(os.path.join(test_base_folder, clip_path.lstrip('/\\\\')))\n",
    "\n",
    "    print(f\"Full Path: {full_clip_path}\")\n",
    "    # full_clip_path = os.path.normpath(os.path.join(test_base_folder, clip_path.lstrip('/\\\\')))\n",
    "    print(f\"Resolved full path: {full_clip_path}\")  # Debugging print\n",
    "\n",
    "    if not os.path.exists(full_clip_path):\n",
    "        raise FileNotFoundError(f\"Clip frames or video not found at {full_clip_path}.\")\n",
    "\n",
    "    return full_clip_path, label\n",
    "\n",
    "\n",
    "\n",
    "clip_name = \"v_Biking_g06_c01\"  # Replace with an actual clip name from the CSV\n",
    "full_path, label = get_clip_paths(clip_name)\n",
    "print(f\"Full Path: {full_path}\")\n",
    "print(f\"Label: {label}\")\n",
    "\n",
    "# Paths for evaluation\n",
    "# input_dir = r'C:\\Users\\Ali Arfa\\Downloads\\deep_learning_project\\test'\n",
    "# target_dir = 'path_to_target'\n",
    "# Example clip to test\n",
    "clip_name = \"v_Biking_g06_c01\"  # Replace with a valid clip name from the CSV\n",
    "output_dir = r'C:\\Users\\Ali Arfa\\Downloads\\deep_learning_project\\results'\n",
    "\n",
    "# Test each model\n",
    "evaluate_model_from_csv(predrnn_model, 'PredRNN', clip_name, output_dir)\n",
    "evaluate_model_from_csv(transformer_model, 'Transformer', clip_name, output_dir)\n",
    "evaluate_model_from_csv(conv_lstm_model, 'ConvLSTM', clip_name, output_dir)\n",
    "\n",
    "# # Test each model\n",
    "# evaluate_model(predrnn_model, 'PredRNN', input_dir, target_dir, output_dir)\n",
    "# evaluate_model(transformer_model, 'Transformer', input_dir, target_dir, output_dir)\n",
    "# evaluate_model(conv_lstm_model, 'ConvLSTM', input_dir, target_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import os\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# from torchvision import transforms\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "\n",
    "# # Helper function to load frames from a directory\n",
    "# def load_frames(directory, transform, num_frames):\n",
    "#     frames = sorted(os.listdir(directory))[:num_frames]\n",
    "#     images = []\n",
    "#     for frame in frames:\n",
    "#         frame_path = os.path.join(directory, frame)\n",
    "#         img = Image.open(frame_path)\n",
    "#         if transform:\n",
    "#             img = transform(img)\n",
    "#         images.append(img)\n",
    "#     return torch.stack(images)\n",
    "\n",
    "# # Helper function to generate a video from frames\n",
    "# def generate_video(frames, output_path, fps=10):\n",
    "#     height, width = frames[0].shape\n",
    "#     video = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height), isColor=False)\n",
    "#     for frame in frames:\n",
    "#         frame = (frame * 255).astype(np.uint8)\n",
    "#         video.write(cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR))\n",
    "#     video.release()\n",
    "#     print(f\"Video saved at {output_path}\")\n",
    "\n",
    "# # Test and evaluate a model\n",
    "# def test_and_evaluate(model, model_name, input_dir, target_dir, output_dir, num_input_frames=10, num_predicted_frames=5):\n",
    "#     model.eval()\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     # Load input and target frames\n",
    "#     input_frames = load_frames(input_dir, transform, num_input_frames).to(device)\n",
    "#     target_frames = load_frames(target_dir, transform, num_predicted_frames)\n",
    "\n",
    "#     # Predict frames\n",
    "#     predictions = []\n",
    "#     current_sequence = input_frames\n",
    "#     with torch.no_grad():\n",
    "#         for _ in range(num_predicted_frames):\n",
    "#             prediction = model(current_sequence.unsqueeze(0))  # Predict next frame\n",
    "#             predictions.append(prediction.cpu().view(64, 64).numpy())  # Reshape prediction\n",
    "#             current_sequence = torch.cat((current_sequence[1:], prediction), dim=0)\n",
    "\n",
    "#     # Evaluate using MSE\n",
    "#     mse_scores = []\n",
    "#     for i in range(num_predicted_frames):\n",
    "#         mse = mean_squared_error(predictions[i].flatten(), target_frames[i][0].cpu().numpy().flatten())\n",
    "#         mse_scores.append(mse)\n",
    "\n",
    "#     # Visualize predictions vs ground truth\n",
    "#     for i, (pred_frame, actual_frame) in enumerate(zip(predictions, target_frames)):\n",
    "#         fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "#         axs[0].imshow(pred_frame, cmap=\"gray\")\n",
    "#         axs[0].set_title(f\"{model_name} Predicted Frame {i + 1}\")\n",
    "#         axs[0].axis(\"off\")\n",
    "#         axs[1].imshow(actual_frame[0].cpu().numpy(), cmap=\"gray\")\n",
    "#         axs[1].set_title(f\"Ground Truth Frame {i + 1}\")\n",
    "#         axs[1].axis(\"off\")\n",
    "\n",
    "#         save_path = os.path.join(output_dir, f\"{model_name}_frame_{i + 1}.png\")\n",
    "#         plt.savefig(save_path)\n",
    "#         plt.close(fig)\n",
    "#         print(f\"Saved visualization: {save_path}\")\n",
    "\n",
    "#     # Generate video from predicted frames\n",
    "#     generate_video(predictions, os.path.join(output_dir, f\"{model_name}_predicted.mp4\"))\n",
    "\n",
    "#     # Return evaluation results\n",
    "#     avg_mse = np.mean(mse_scores)\n",
    "#     print(f\"{model_name} Average MSE: {avg_mse:.4f}\")\n",
    "#     return avg_mse\n",
    "\n",
    "# # Load models (define your model loading logic here)\n",
    "# def load_model(model_class, state_path, **model_kwargs):\n",
    "#     model = model_class(**model_kwargs).to(device)\n",
    "#     model.load_state_dict(torch.load(state_path))\n",
    "#     model.eval()\n",
    "#     print(f\"{model_class.__name__} model loaded successfully.\")\n",
    "#     return model\n",
    "\n",
    "# # Configuration\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Grayscale(),\n",
    "#     transforms.Resize((64, 64)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "# ])\n",
    "# input_dir = \"path_to_input_frames\"\n",
    "# target_dir = \"path_to_target_frames\"\n",
    "# output_base_dir = \"model_comparison_results\"\n",
    "\n",
    "# # Define models and paths\n",
    "# models = [\n",
    "#     {\n",
    "#         \"name\": \"PredRNN\",\n",
    "#         \"class\": PredRNN,\n",
    "#         \"state_path\": \"predrnn_model_state.pth\",\n",
    "#         \"kwargs\": {\"input_dim\": 64 * 64, \"hidden_dim\": 512, \"output_dim\": 64 * 64, \"num_layers\": 2}\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"ConvLSTM\",\n",
    "#         \"class\": ConvLSTM,  # Assume this is defined elsewhere\n",
    "#         \"state_path\": \"convlstm_model_state.pth\",\n",
    "#         \"kwargs\": {\"input_dim\": 64 * 64, \"hidden_dim\": 512, \"output_dim\": 64 * 64, \"num_layers\": 2}\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"Transformer\",\n",
    "#         \"class\": TransformerFramePredictor,\n",
    "#         \"state_path\": \"transformer_model_state.pth\",\n",
    "#         \"kwargs\": {\"input_dim\": 64 * 64, \"num_heads\": 8, \"num_layers\": 4, \"hidden_dim\": 512, \"max_seq_length\": 15}\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# # Evaluate all models\n",
    "# results = {}\n",
    "# for model_config in models:\n",
    "#     model_name = model_config[\"name\"]\n",
    "#     output_dir = os.path.join(output_base_dir, model_name)\n",
    "#     model = load_model(model_config[\"class\"], model_config[\"state_path\"], **model_config[\"kwargs\"])\n",
    "\n",
    "#     avg_mse = test_and_evaluate(\n",
    "#         model=model,\n",
    "#         model_name=model_name,\n",
    "#         input_dir=input_dir,\n",
    "#         target_dir=target_dir,\n",
    "#         output_dir=output_dir,\n",
    "#         num_input_frames=10,\n",
    "#         num_predicted_frames=5\n",
    "#     )\n",
    "#     results[model_name] = avg_mse\n",
    "\n",
    "# # Display final results\n",
    "# print(\"\\nFinal Evaluation Results:\")\n",
    "# for model_name, mse in results.items():\n",
    "#     print(f\"{model_name}: Average MSE = {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# # Define a function to load frames from a directory\n",
    "# def load_frames(directory, transform, num_frames):\n",
    "#     frames = sorted(os.listdir(directory))[:num_frames]\n",
    "#     images = []\n",
    "#     for frame in frames:\n",
    "#         frame_path = os.path.join(directory, frame)\n",
    "#         img = Image.open(frame_path)\n",
    "#         if transform:\n",
    "#             img = transform(img)\n",
    "#         images.append(img)\n",
    "#     return torch.stack(images)\n",
    "\n",
    "# # Test and evaluate a model\n",
    "# def test_and_evaluate(model, model_name, input_dir, target_dir, output_dir, num_input_frames=10, num_predicted_frames=5):\n",
    "#     model.eval()\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     # Load input and target frames\n",
    "#     input_frames = load_frames(input_dir, transform, num_input_frames).to(device)\n",
    "#     target_frames = load_frames(target_dir, transform, num_predicted_frames)\n",
    "\n",
    "#     # Predict frames\n",
    "#     predictions = []\n",
    "#     current_sequence = input_frames\n",
    "#     with torch.no_grad():\n",
    "#         for _ in range(num_predicted_frames):\n",
    "#             prediction = model(current_sequence.unsqueeze(0))  # Predict next frame\n",
    "#             predictions.append(prediction.cpu().view(64, 64).numpy())  # Reshape prediction\n",
    "#             current_sequence = torch.cat((current_sequence[1:], prediction), dim=0)\n",
    "\n",
    "#     # Evaluate using MSE\n",
    "#     mse_scores = []\n",
    "#     for i in range(num_predicted_frames):\n",
    "#         mse = mean_squared_error(predictions[i].flatten(), target_frames[i][0].cpu().numpy().flatten())\n",
    "#         mse_scores.append(mse)\n",
    "\n",
    "#     # Visualize predictions vs ground truth\n",
    "#     for i, (pred_frame, actual_frame) in enumerate(zip(predictions, target_frames)):\n",
    "#         fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "#         axs[0].imshow(pred_frame, cmap=\"gray\")\n",
    "#         axs[0].set_title(f\"{model_name} Predicted Frame {i + 1}\")\n",
    "#         axs[0].axis(\"off\")\n",
    "#         axs[1].imshow(actual_frame[0].cpu().numpy(), cmap=\"gray\")\n",
    "#         axs[1].set_title(f\"Ground Truth Frame {i + 1}\")\n",
    "#         axs[1].axis(\"off\")\n",
    "\n",
    "#         save_path = os.path.join(output_dir, f\"{model_name}_frame_{i + 1}.png\")\n",
    "#         plt.savefig(save_path)\n",
    "#         plt.close(fig)\n",
    "#         print(f\"Saved visualization: {save_path}\")\n",
    "\n",
    "#     # Return evaluation results\n",
    "#     avg_mse = np.mean(mse_scores)\n",
    "#     print(f\"{model_name} Average MSE: {avg_mse:.4f}\")\n",
    "#     return avg_mse\n",
    "\n",
    "# # Load models\n",
    "# def load_model(model_class, state_path, **model_kwargs):\n",
    "#     model = model_class(**model_kwargs).to(device)\n",
    "#     model.load_state_dict(torch.load(state_path))\n",
    "#     model.eval()\n",
    "#     print(f\"{model_class.__name__} model loaded successfully.\")\n",
    "#     return model\n",
    "\n",
    "# # Configuration\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Grayscale(),\n",
    "#     transforms.Resize((64, 64)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "# ])\n",
    "# input_dir = \"path_to_input_frames\"\n",
    "# target_dir = \"path_to_target_frames\"\n",
    "# output_base_dir = \"model_comparison_results\"\n",
    "\n",
    "# # Define models and paths\n",
    "# models = [\n",
    "#     {\n",
    "#         \"name\": \"PredRNN\",\n",
    "#         \"class\": PredRNN,\n",
    "#         \"state_path\": \"predrnn_model_state.pth\",\n",
    "#         \"kwargs\": {\"input_dim\": 64 * 64, \"hidden_dim\": 512, \"output_dim\": 64 * 64, \"num_layers\": 2}\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"ConvLSTM\",\n",
    "#         \"class\": ConvLSTM,  # Assume this is defined elsewhere\n",
    "#         \"state_path\": \"convlstm_model_state.pth\",\n",
    "#         \"kwargs\": {\"input_dim\": 64 * 64, \"hidden_dim\": 512, \"output_dim\": 64 * 64, \"num_layers\": 2}\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"Transformer\",\n",
    "#         \"class\": TransformerFramePredictor,\n",
    "#         \"state_path\": \"transformer_model_state.pth\",\n",
    "#         \"kwargs\": {\"input_dim\": 64 * 64, \"num_heads\": 8, \"num_layers\": 4, \"hidden_dim\": 512, \"max_seq_length\": 15}\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# # Evaluate all models\n",
    "# results = {}\n",
    "# for model_config in models:\n",
    "#     model_name = model_config[\"name\"]\n",
    "#     output_dir = os.path.join(output_base_dir, model_name)\n",
    "#     model = load_model(model_config[\"class\"], model_config[\"state_path\"], **model_config[\"kwargs\"])\n",
    "\n",
    "#     avg_mse = test_and_evaluate(\n",
    "#         model=model,\n",
    "#         model_name=model_name,\n",
    "#         input_dir=input_dir,\n",
    "#         target_dir=target_dir,\n",
    "#         output_dir=output_dir,\n",
    "#         num_input_frames=10,\n",
    "#         num_predicted_frames=5\n",
    "#     )\n",
    "#     results[model_name] = avg_mse\n",
    "\n",
    "# # Display final results\n",
    "# print(\"\\nFinal Evaluation Results:\")\n",
    "# for model_name, mse in results.items():\n",
    "#     print(f\"{model_name}: Average MSE = {mse:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
